/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.5.3 public/models/avatar.glb 
*/

import React, { useEffect, useRef, useState } from "react";
import { useFrame, useGraph } from "@react-three/fiber";
import { useAnimations, useFBX, useGLTF } from "@react-three/drei";
import { SkeletonUtils } from "three-stdlib";
import * as THREE from "three";
import visemeQueue from "../../public/audio/api_0.json";

const corresponding = {
  A: "viseme_PP",
  B: "viseme_kk",
  C: "viseme_I",
  D: "viseme_AA",
  E: "viseme_O",
  F: "viseme_U",
  G: "viseme_FF",
  H: "viseme_TH",
  X: "viseme_PP",
};

const setupMode = true;

export function Avatar(props) {
  const group = useRef();
  const { scene } = useGLTF("models/avatar.glb");
  const clone = React.useMemo(() => SkeletonUtils.clone(scene), [scene]);
  const { nodes, materials } = useGraph(clone);

  // Load animations
  const { animations: idleAnimation } = useFBX(
    "animations/idles/Breathing Idle.fbx"
  );
  const { animations: talkingAnimation } = useFBX("animations/Talking.fbx");
  idleAnimation[0].name = "Standing";
  talkingAnimation[0].name = "Talking";

  const { actions, mixer } = useAnimations(
    [...idleAnimation, ...talkingAnimation],
    group
  );

  // Store mouth cues and audio reference
  const [mouthCues, setMouthCues] = useState([]);
  const audioRef = useRef(null);
  const [isTalking, setIsTalking] = useState(false);
  const [animation, setAnimation] = useState("Standing");

  useEffect(() => {
    setMouthCues(visemeQueue["mouthCues"]);
  }, []);

  useEffect(() => {
    const handleKeyPress = (event) => {
      if (event.code === "Space") {
        setIsTalking((prev) => !prev);
        setAnimation((prev) => (prev === "Standing" ? "Talking" : "Standing"));
      }
    };

    window.addEventListener("keydown", handleKeyPress);
    return () => window.removeEventListener("keydown", handleKeyPress);
  }, []);

  useEffect(() => {
    if (isTalking) {
      // Create and play new audio instance
      const audio = new Audio("../../public/audio/api_0.wav");
      audioRef.current = audio;
      audio.play().catch((error) => console.error("Audio play failed:", error));
      audio.onended = () => {
        setIsTalking(false);
        setAnimation("Standing");
      };
    }
  }, [isTalking]);

  useEffect(() => {
    actions[animation]?.reset().fadeIn(0.5).play();
    return () => actions[animation]?.fadeOut(0.5);
  }, [animation, actions]);

  const lerpMorphTarget = (target, value, speed = 0.1) => {
    clone.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const index = child.morphTargetDictionary[target];
        if (index === undefined) return;
        child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
          child.morphTargetInfluences[index],
          value,
          speed
        );
      }
    });
  };

  useFrame(() => {
    const audio = audioRef.current;
    if (audio && !audio.paused) {
      const currentTime = audio.currentTime;

      // Find the current viseme based on audio time
      const currentCue = mouthCues.find(
        (cue) => currentTime >= cue.start && currentTime <= cue.end
      );

      if (currentCue) {
        lerpMorphTarget(corresponding[currentCue.value], 1, 0.1);
      }
    }
  });

  return (
    <group {...props} ref={group} dispose={null}>
      <group rotation-x={-Math.PI / 2}>
        <primitive object={nodes.Hips} />
        <skinnedMesh
          name="Wolf3D_Avatar"
          geometry={nodes.Wolf3D_Avatar.geometry}
          material={materials.Wolf3D_Avatar}
          skeleton={nodes.Wolf3D_Avatar.skeleton}
          morphTargetDictionary={nodes.Wolf3D_Avatar.morphTargetDictionary}
          morphTargetInfluences={nodes.Wolf3D_Avatar.morphTargetInfluences}
        />
      </group>
    </group>
  );
}

useGLTF.preload("models/avatar.glb");
