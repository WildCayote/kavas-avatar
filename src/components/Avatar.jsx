/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.5.3 public/models/avatar.glb 
*/

import React, { useEffect, useRef, useState } from "react";
import { useFrame, useGraph, useThree } from "@react-three/fiber";
import { useAnimations, useFBX, useGLTF } from "@react-three/drei";
import { SkeletonUtils } from "three-stdlib";
import * as THREE from "three";
import visemeQueue from "../../public/audio/3.json";

const corresponding = {
  A: "viseme_PP",
  B: "viseme_kk",
  C: "viseme_I",
  D: "viseme_AA",
  E: "viseme_O",
  F: "viseme_U",
  G: "viseme_FF",
  H: "viseme_TH",
  X: "viseme_PP",
};

const setupMode = true;

export function Avatar(props) {
  const group = useRef();
  const { scene } = useGLTF("models/avatar.glb");
  const clone = React.useMemo(() => SkeletonUtils.clone(scene), [scene]);
  const { nodes, materials } = useGraph(clone);
  const { camera } = useThree();
  const [position, setPosition] = useState();

  useEffect(() => {
    // Set fixed camera position
    camera.position.set(
      0.009717560321996537,
      -7.5977023380699285,
      3.83839987900298
    );

    // Set fixed camera rotation
    camera.rotation.set(
      1.1029931132540758,
      0.0011415966961690726,
      -0.0022596643170388585
    );
  }, [camera]);

  // Load animations
  const { animations: idleAnimation } = useFBX(
    "animations/idles/Breathing Idle.fbx"
  );
  const { animations: talkingAnimation } = useFBX("animations/Talking.fbx");
  idleAnimation[0].name = "Standing";
  talkingAnimation[0].name = "Talking";

  const { actions } = useAnimations(
    [idleAnimation[0], talkingAnimation[0]],
    group
  );

  // Store mouth cues and audio reference
  const [mouthCues, setMouthCues] = useState([]);
  const audioRef = useRef(null);
  const [isTalking, setIsTalking] = useState(false);
  const [animation, setAnimation] = useState("Standing");

  useEffect(() => {
    setMouthCues(visemeQueue["mouthCues"]);
  }, []);

  useEffect(() => {
    const handleKeyPress = (event) => {
      if (event.code === "Space" && !isTalking) {
        setIsTalking((prev) => !prev);
        setAnimation((prev) => (prev === "Standing" ? "Talking" : "Standing"));
      }
    };

    window.addEventListener("keydown", handleKeyPress);
    return () => window.removeEventListener("keydown", handleKeyPress);
  }, [isTalking]);

  useEffect(() => {
    if (isTalking) {
      // Create and play new audio instance
      const audio = new Audio("../../public/audio/3.wav");
      audioRef.current = audio;
      audio.play().catch((error) => console.error("Audio play failed:", error));
      audio.onended = () => {
        setIsTalking(false);
        setAnimation("Standing");
      };
    }
  }, [isTalking]);

  useEffect(() => {
    actions[animation]?.reset().fadeIn(0.5).play();
    group.current.position.set(0, -1.5, 2);
    return () => actions[animation]?.fadeOut(0.5);
  }, [actions]);

  const lerpMorphTarget = (target, value, speed = 0.1) => {
    clone.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const index = child.morphTargetDictionary[target];
        if (index === undefined) return;
        child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
          child.morphTargetInfluences[index],
          value,
          speed
        );
      }
    });
  };

  useFrame(() => {
    const audio = audioRef.current;
    if (audio && !audio.paused) {
      const currentTime = audio.currentTime;

      // Find the current viseme based on audio time
      const currentCue = mouthCues.find(
        (cue) => currentTime >= cue.start && currentTime <= cue.end
      );

      // Reset all morph targets to 0 before applying the current viseme
      clone.traverse((child) => {
        if (child.isSkinnedMesh && child.morphTargetDictionary) {
          Object.keys(child.morphTargetDictionary).forEach((target) => {
            const index = child.morphTargetDictionary[target];
            if (index !== undefined) {
              child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
                child.morphTargetInfluences[index],
                0,
                0.1
              );
            }
          });
        }
      });

      // Apply the current viseme blendshape if one is found
      if (currentCue) {
        lerpMorphTarget(corresponding[currentCue.value], 1, 0.1);
      }
    }
  });

  return (
    <group {...props} ref={group} dispose={null}>
      <group rotation-x={-Math.PI / 2}>
        <primitive object={nodes.Hips} />
        <skinnedMesh
          name="Wolf3D_Avatar"
          geometry={nodes.Wolf3D_Avatar.geometry}
          material={materials.Wolf3D_Avatar}
          skeleton={nodes.Wolf3D_Avatar.skeleton}
          morphTargetDictionary={nodes.Wolf3D_Avatar.morphTargetDictionary}
          morphTargetInfluences={nodes.Wolf3D_Avatar.morphTargetInfluences}
        />
      </group>
    </group>
  );
}

useGLTF.preload("models/avatar.glb");
